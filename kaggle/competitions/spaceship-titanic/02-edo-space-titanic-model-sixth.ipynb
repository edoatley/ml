{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "source": [
    "# Spaceship Titanic - third Model\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, \n",
    "the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets \n",
    "orbiting nearby stars.\n",
    "\n",
    "While rounding Alpha Centauri en route to its first destination—the torrid 55 Cancri E—the unwary Spaceship Titanic\n",
    "collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000\n",
    "years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!\n",
    "\n",
    "In this competition your task is to predict whether a passenger was transported to an alternate dimension during the \n",
    "Spaceship Titanic's collision with the spacetime anomaly. To help you make these predictions, you're given a set of personal\n",
    "records recovered from the ship's damaged computer system.\n",
    "\n",
    "## File and Data Field Descriptions\n",
    "\n",
    "### train.csv \n",
    "\n",
    "Personal records for about two-thirds (~8700) of the passengers, to be used as training data.\n",
    "\n",
    "Sure! Here's the code converted to a markdown table:\n",
    "\n",
    "| Column Name | Description |\n",
    "|------------- |-------------|\n",
    "| `PassengerId` | A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always. |\n",
    "| `HomePlanet` | The planet the passenger departed from, typically their planet of permanent residence. |\n",
    "| `CryoSleep` | Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins. |\n",
    "| `Cabin` | The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard. |\n",
    "| `Destination` | The planet the passenger will be debarking to. |\n",
    "| `Age` | The age of the passenger. |\n",
    "| `VIP` | Whether the passenger has paid for special VIP service during the voyage. |\n",
    "| `RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck` | Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities. |\n",
    "| `Name` | The first and last names of the passenger. |\n",
    "| `Transported` | Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict. |\n",
    "\n",
    "### test.csv\n",
    "\n",
    "Personal records for the remaining one-third (~4300) of the passengers, to be used as test data. \n",
    "\n",
    "Your task is to predict the value of Transported for the passengers in this set.\n",
    "    \n",
    "### sample_submission.csv\n",
    "\n",
    "A sample submission file in the correct format.\n",
    "\n",
    "| Column Name | Description |\n",
    "|------------- |-------------|\n",
    "| `PassengerId` | A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always. |\n",
    "| `Transported` | Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='sixth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "\n",
    "# Data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno\n",
    "from collections import Counter\n",
    "\n",
    "# Data visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "## Drop unrequired features & add engineered ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data - to pandas dataframes\n",
    "\n",
    "test_df = pd.read_csv('./inputs/test.csv')\n",
    "test_idx = test_df['PassengerId']\n",
    "train_df = pd.read_csv('./inputs/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split name into components\n",
    "def split_name(name:str):\n",
    "    if pd.isnull(name): # protect against empty values\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    parts = name.split(' ')\n",
    "    first = parts[0]\n",
    "    last = parts[-1]\n",
    "    return (first, last)\n",
    "\n",
    "# split cabin into components\n",
    "def split_cabin(name:str):\n",
    "    if pd.isnull(name): # protect against empty values\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    parts = name.split('/')\n",
    "    deck = parts[0]\n",
    "    side = parts[-1]\n",
    "    number = ' '.join(parts[1:-1])\n",
    "    return (deck, number, side)\n",
    "\n",
    "# This function will drop and add required features\n",
    "def feature_manipulation(d : pd.DataFrame) -> pd.DataFrame:\n",
    "    # print the shape of the provided dataframe\n",
    "    print(\"Before: \", d.shape)\n",
    "    # Clone the provided dataframe\n",
    "    df = d.copy()\n",
    "\n",
    "    _, df['Name'] = zip(*df['Name'].map(split_name))\n",
    "    \n",
    "    # Location\n",
    "    df['Deck'], _, df['Side'] = zip(*df['Cabin'].map(split_cabin))\n",
    "    df['DeckSide'] = df['Deck'] + df['Side'] # Combine Deck and Side\n",
    "\n",
    "    # Group related columns\n",
    "    df['Group'] = df['PassengerId'].map(lambda x: x.split('_')[0])\n",
    "    df['GroupSize'] = df['Group'].map(df['Group'].value_counts())\n",
    "    \n",
    "    print(\"After: \", df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = feature_manipulation(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = feature_manipulation(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Data\n",
    "\n",
    "In the first model we had a complicated approach - here we will just use the median for numeric and mode for non-numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Before: {len(train_df)} rows')\n",
    "train_df.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Before: {len(test_df)} rows')\n",
    "train_df.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows with no missing values\n",
    "print(f'Before: {len(train_df)} rows')\n",
    "train_nona = train_df.dropna()\n",
    "print(f'After: {len(train_nona)} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the empty data handling\n",
    "\n",
    "### Define functions to replace data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_median(df, features):\n",
    "    for feature in features:\n",
    "        df[feature] = df[feature].fillna(df[feature].median())\n",
    "    return df\n",
    "\n",
    "def fill_with_mode(df, features):\n",
    "    for feature in features:\n",
    "        df[feature] = df[feature].fillna(df[feature].mode()[0])\n",
    "    return df\n",
    "\n",
    "def fill_with_mean(df, features):\n",
    "    for feature in features:\n",
    "        df[feature] = df[feature].fillna(df[feature].mean())\n",
    "    return df\n",
    "\n",
    "# Here we provide a list of other_features - if a row has a nan in a feature we will fill it with the mean of the other_features in its row \n",
    "def fill_with_mean_of_other_features(df, features, other_features):\n",
    "    for feature in features:\n",
    "        df[feature] = df[feature].fillna(df[other_features].dropna().mean(axis=1))\n",
    "    return df\n",
    "\n",
    "def fill_with_constant(df, features, constant):\n",
    "    for feature in features:\n",
    "        df[feature] = df[feature].fillna(constant)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply simple rules for empty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_empty_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "   \n",
    "    # fill in missing values with a constant\n",
    "    df = fill_with_constant(df, ['VIP'], False)\n",
    "    df = fill_with_constant(df, ['Name', 'Cabin', 'Deck', 'Side', 'DeckSide', 'HomePlanet', 'Destination'], 'Unknown')\n",
    "\n",
    "    # Fill numerics with median\n",
    "    df = fill_with_median(df, ['Age'])\n",
    "    \n",
    "    # Fill with average of other spend features\n",
    "    spend_features = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    df = fill_with_mean_of_other_features(df, spend_features, spend_features)\n",
    "    df = fill_with_constant(df, spend_features, 0)\n",
    "\n",
    "    # Have a sensible default so leave this as is\n",
    "    # for the provided dataframe set CryoSleep to False if TotalSpend is greater than 0\n",
    "    UnknownCryoSpender = (df[\"CryoSleep\"].isnull() | df[\"CryoSleep\"].isna()) & (df[spend_features].dropna().sum(axis=1) > 0)\n",
    "    df.loc[UnknownCryoSpender, 'CryoSleep'] = False\n",
    "    df['CryoSleep'] = df['CryoSleep'].fillna('Unknown')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = handle_empty_data(train_df)\n",
    "test_df = handle_empty_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the training data\n",
    "\n",
    "train_df.isnull().sum().sort_values(ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the test data\n",
    "test_df.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And thats a wrap!\n",
    "\n",
    "There are now no missing values so we can proceed to Pre-processing\n",
    "\n",
    "# Data Preprocessing\n",
    "\n",
    "## Quick check of the data after the empty data is filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the training data after the changes to remove empty data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the test data\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes:\n",
    "\n",
    "print(f'Train: {train_df.shape}')\n",
    "print(f'Test: {test_df.shape}')\n",
    "\n",
    "# Check the train and test column data types side by side\n",
    "pd.concat([train_df.dtypes, test_df.dtypes], axis=1, keys=['Train', 'Test'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Unrequired columns\n",
    "\n",
    "We can now drop these columns:\n",
    "\n",
    "- Cabin\n",
    "- TotalSpend\n",
    "- PassengerId - may do this one last so we have the key\n",
    "\n",
    "And to simplify for now lets also remove\n",
    "- Name\n",
    "- Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns that are not required\n",
    "def drop_unrequired_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    columns_to_drop = [\n",
    "        'Cabin',\n",
    "        'Name',\n",
    "        'DeckSide'\n",
    "    ]\n",
    "    df.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "    return df\n",
    "\n",
    "train_df = drop_unrequired_columns(train_df)\n",
    "test_df = drop_unrequired_columns(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for training\n",
    "\n",
    "- Log/Bin the numeric data\n",
    "- OneHotEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin and OHE features\n",
    "\n",
    "I am going to try the `KBinsDiscretizer` from sklearn - see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Define pipelines for the features where we wish to bin and OHE\n",
    "\n",
    "spending_pipeline = Pipeline([\n",
    "    ('scale', RobustScaler()),\n",
    "])\n",
    "\n",
    "age_pipeline = Pipeline([\n",
    "    ('binning', KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some cases where we have used 'Unknown' in boolean columns that have True and False otherwise that led to an error\n",
    "# complaining about a mix of str and bool so we need to convert the columns to str\n",
    "\n",
    "# print all columns where the value is 'Unknown'\n",
    "for column in train_df.columns:\n",
    "    if 'Unknown' in train_df[column].unique():\n",
    "        print(column)\n",
    "        train_df[column] = train_df[column].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what data looks like before pipeline\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessor\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "spending_features = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'] # bin then OHE\n",
    "categorical_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side'] # OHE\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('category', categorical_pipeline, categorical_features),\n",
    "        ('spending', spending_pipeline, spending_features),\n",
    "        ('age', age_pipeline, ['Age'])\n",
    "    ],\n",
    "    remainder='passthrough'  # This leaves the rest of the columns untouched\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df.pop('Transported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = train_df.pop('PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can fit and transform the training data with the preprocessor\n",
    "X = preprocessor.fit_transform(train_df) # sparse to dense array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to attempt the modelling\n",
    "\n",
    "## Split the data to create a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorFlow classifier\n",
    "def tensorflow_model():\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'Perceptron': Perceptron(),\n",
    "    'SVC': SVC(),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors = 5),\n",
    "    'DecisionTree': DecisionTreeClassifier(),\n",
    "    'Gaussian': GaussianNB(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'LinearSvc': LinearSVC(),\n",
    "    'SGDClassifier': SGDClassifier(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models\n",
    "\n",
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(classifier, X_train, X_val, y_train, y_val):\n",
    "    # Fit the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the test data\n",
    "    y_pred = classifier.predict(X_val)\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    scores = cross_val_score(classifier, X_train, y_train, cv = 10)\n",
    "\n",
    "    return { \n",
    "            'confusion_matrix': cm,\n",
    "            'accuracy': acc,\n",
    "            'kfold-cv': scores.mean()\n",
    "        }\n",
    "\n",
    "def run_classifiers(classifiers: dict, X_train, X_test, y_train, y_val) -> dict:\n",
    "    results = {}\n",
    "    for name, classifier in classifiers.items():\n",
    "        results[name] = score_model(classifier, X_train, X_test, y_train, y_val)\n",
    "    return results\n",
    "\n",
    "results = run_classifiers(classifiers, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert results to a dataframe\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df.sort_values(by='kfold-cv', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the two best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the confusion matrix with labels for TP, FP, TN, FN\n",
    "def print_confusion_matrix(cm: np.ndarray):\n",
    "    # True Positives\n",
    "    TP = cm[1, 1]\n",
    "    # True Negatives\n",
    "    TN = cm[0, 0]\n",
    "    # False Positives\n",
    "    FP = cm[0, 1]\n",
    "    # False Negatives\n",
    "    FN = cm[1, 0]\n",
    "\n",
    "    # Now we have extracted the values print as a grid\n",
    "    print(f'True Positives: {TP} False Positives: {FP}')\n",
    "    print(f'False Negatives: {FN} True Negatives: {TN}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the two best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune a model using GridSearchCV\n",
    "def tune_model(classifier, param_grid):\n",
    "    print(f'\\n\\nTuning {classifier.__class__.__name__}...')\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=10, n_jobs = -1, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    score = score_model(best_estimator, X_train, X_val, y_train, y_val)\n",
    "\n",
    "    print(f'Best SVC Accuracy: { score[\"accuracy\"] }')\n",
    "    print(f'Best SVC KFold CV: { score[\"kfold-cv\"] }')\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print_confusion_matrix(score['confusion_matrix'])\n",
    "    \n",
    "    f1 = f1_score(y_val, best_estimator.predict(X_val))\n",
    "    # and finally the f1 score\n",
    "    print(f'F1 Score: {f1}')\n",
    "\n",
    "    return (grid_search.best_params_, best_estimator, f1)\n",
    "\n",
    "# Print the confusion matrix with labels for TP, FP, TN, FN\n",
    "def print_confusion_matrix(cm: np.ndarray):\n",
    "    # True Positives\n",
    "    TP = cm[1, 1]\n",
    "    # True Negatives\n",
    "    TN = cm[0, 0]\n",
    "    # False Positives\n",
    "    FP = cm[0, 1]\n",
    "    # False Negatives\n",
    "    FN = cm[1, 0]\n",
    "\n",
    "    # Now we have extracted the values print as a grid\n",
    "    print(f'True Positives: {TP} False Positives: {FP}')\n",
    "    print(f'False Negatives: {FN} True Negatives: {TN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results = [\n",
    "    tune_model(\n",
    "        XGBClassifier(),\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'learning_rate': [0.1, 0.01, 0.001],\n",
    "            'eval_metric': ['logloss'], \n",
    "            'use_label_encoder': [False]\n",
    "        }, \n",
    "    ),\n",
    "    tune_model(\n",
    "        RandomForestClassifier(), \n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'criterion': ['gini', 'entropy']\n",
    "        }\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = max(tuning_results, key=lambda x: x[2])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission\n",
    "\n",
    "Now we can predict the test set and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert any Boolean columns with Unknown to String type\n",
    "for column in test_df.columns:\n",
    "    if 'Unknown' in test_df[column].unique():\n",
    "        print(column)\n",
    "        test_df[column] = test_df[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "\n",
    "print(f'Test: {test_df.shape}')\n",
    "\n",
    "# Print the first 5 rows:\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the passenger IDs for submission and remove from the data as not useful predictors\n",
    "idx_test = test_df.pop('PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process test set\n",
    "X_test = preprocessor.transform(test_df) # sparse to dense array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set\n",
    "y_pred = best_result.predict(X_test).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the submission DataFrame\n",
    "submission = pd.DataFrame({'PassengerId': idx_test, 'Transported': y_pred})\n",
    "submission.to_csv(f'submissions/{model}-submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
