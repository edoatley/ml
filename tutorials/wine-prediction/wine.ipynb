{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Set up your environment.\n",
    "\n",
    "This is the tutorial defined [here](https://elitedatascience.com/python-machine-learning-tutorial-scikit-learn#step-1). To get started we set up our environment:\n",
    "\n",
    "1. Create `venv` (or do so in vs code):\n",
    "\n",
    "```bash\n",
    "python3 -m venv .venv\n",
    "```\n",
    "\n",
    "2. Activate `venv` (path is relative to this file):\n",
    "\n",
    "```bash\n",
    "source ../../.venv/bin/activate \n",
    "```\n",
    "\n",
    "3. Check python & pip are there and using venv ones:\n",
    "\n",
    "```bash\n",
    "which python \n",
    "which pip \n",
    "```\n",
    "\n",
    "4. Install packages:\n",
    "\n",
    "```bash\n",
    "pip install scikit-learn\n",
    "pip install numpy \n",
    "pip install pandas\n",
    "```\n",
    "\n",
    "5. Freeze packages and write `requirements.txt`:\n",
    "\n",
    "```bash\n",
    "pip freeze > requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 2: Import libraries and modules.\n",
    "\n",
    "# import numpy, which provides support for more efficient numerical computation:\n",
    "import numpy as np\n",
    "\n",
    "# Pandas, a convenient library that supports dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# model_selection - contains many utilities that will help us choose between models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# preprocessing module. This contains utilities for scaling, transforming, and wrangling data.\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# import the families of models we’ll need - random forest family\n",
    "# For the scope of this tutorial, we’ll only focus on training a random forest and tuning its parameters. \n",
    "# We’ll have another detailed tutorial for how to choose between model families.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# importing the tools to help us perform cross-validation.\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#some metrics we can use to evaluate our model performance later.\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# way to persist our model for future use - Joblib is an alternative to Python’s pickle package, \n",
    "# and we’ll use it because it’s more efficient for storing large numpy arrays.\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    }
   ],
   "source": [
    "#@title Step 3: Load red wine data.\n",
    "\n",
    "# convenient tool we’ll use today is the read_csv() function. Using this function, we can load any CSV file, even from a remote URL\n",
    "#dataset_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "dataset_url='wine-quality.csv' # using this as actual URL gave self signed SSL error\n",
    "data = pd.read_csv(dataset_url, sep=';') # data is using ; to separate data (not comma default)\n",
    "\n",
    "# Now let’s take a look at the first 5 rows of data:\n",
    "print( data.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 4: Split data into training and test sets.\n",
    "\n",
    "# First, let’s separate our target (y) features from our input (X) features:\n",
    "y = data.quality\n",
    "X = data.drop('quality', axis=1)\n",
    "\n",
    "# This allows us to take advantage of Scikit-Learn’s useful train_test_split function:\n",
    "# As you can see, we’ll set aside 20% of the data as a test set for evaluating our model. We also set an arbitrary “random state” (a.k.a. seed) so that we can reproduce our results.\n",
    "# Finally, it’s good practice to stratify your sample by the target variable. This will ensure your training set looks similar to your test set, making your evaluation metrics more reliable.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123, \n",
    "                                                    stratify=y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Declare data preprocessing steps.\n",
    "\n",
    "Remember, in Step 3, we made the mental note to standardize our features because they were on different scales.\n",
    "\n",
    "### WTF is standardization?\n",
    "\n",
    "Standardization is the process of subtracting the means from each feature and then dividing by the feature standard deviations.\n",
    "\n",
    "Standardization is a common requirement for machine learning tasks. Many algorithms assume that all features are centered around zero and have approximately the same variance.\n",
    "\n",
    "First, here’s some code that we won’t use…\n",
    "\n",
    "Scikit-Learn makes data preprocessing a breeze. For example, it’s pretty easy to simply scale a dataset:\n",
    "\n",
    "```python\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "print( X_train_scaled )\n",
    "# array([[ 0.51358886,  2.19680282, -0.164433  , ...,  1.08415147,\n",
    "#         -0.69866131, -0.58608178],\n",
    "#        [-1.73698885, -0.31792985, -0.82867679, ...,  1.46964764,\n",
    "#          1.2491516 ,  2.97009781],\n",
    "#        [-0.35201795,  0.46443143, -0.47100705, ..., -0.13658641,\n",
    "# ...\n",
    "```\n",
    "\n",
    "You can confirm that the scaled dataset is indeed centered at zero, with unit variance:\n",
    "\n",
    "```Python\n",
    "print( X_train_scaled.mean(axis=0) )\n",
    "# [ 0. -0. -0. -0.  0. -0. -0. -0. -0. -0. -0.]\n",
    "\n",
    "print( X_train_scaled.std(axis=0) )\n",
    "# [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
    "```\n",
    "\n",
    "Great, but why did we say that we won’t use this code?\n",
    "\n",
    "The reason is that we won’t be able to perform the exact same transformation on the test set.\n",
    "\n",
    "Sure, we can still scale the test set separately, but we won’t be using the same means and standard deviations as we used to transform the training set.\n",
    "\n",
    "In other words, that means it wouldn’t be a fair representation of how the model pipeline, include the preprocessing steps, would perform on brand new data.\n",
    "\n",
    "So instead of directly invoking the scale function, we’ll be using a feature in Scikit-Learn called the Transformer API. The Transformer API allows you to “fit” a preprocessing step using the training data the same way you’d fit a model…\n",
    "\n",
    "…and then use the same transformation on future data sets!\n",
    "\n",
    "Here’s what that process looks like:\n",
    "\n",
    "1, Fit the transformer on the training set (saving the means and standard deviations)\n",
    "2. Apply the transformer to the training set (scaling the training data)\n",
    "3. Apply the transformer to the test set (using the same means and standard deviations)\n",
    "\n",
    "This makes your final estimate of model performance more realistic, and it allows to insert your preprocessing steps into a cross-validation pipeline (more on this in Step 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.16664562e-16 -3.05550043e-17 -8.47206937e-17 -2.22218213e-17\n",
      "  2.77772766e-18 -6.38877362e-17 -4.16659149e-18 -1.20753377e-13\n",
      " -8.70817622e-16 -4.08325966e-16 -1.16664562e-15]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# save means and standard deviations for each feature in the training set in scaler object\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "print(X_train_scaled.mean(axis=0))\n",
    "print(X_train_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02776704  0.02592492 -0.03078587 -0.03137977 -0.00471876 -0.04413827\n",
      " -0.02414174 -0.00293273 -0.00467444 -0.10894663  0.01043391]\n",
      "[1.02160495 1.00135689 0.97456598 0.91099054 0.86716698 0.94193125\n",
      " 1.03673213 1.03145119 0.95734849 0.83829505 1.0286218 ]\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(X_test_scaled.mean(axis=0))\n",
    "print(X_test_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice, when we set up the cross-validation pipeline, we won’t even need to manually fit the \n",
    "# Transformer API. Instead, we’ll simply declare the class object, like so:\n",
    "\n",
    "pipeline = make_pipeline(preprocessing.StandardScaler(),\n",
    "                         RandomForestRegressor(n_estimators=100,\n",
    "                                               random_state=123))\n",
    "\n",
    "# This is exactly what it looks like: a modeling pipeline that first transforms the data using StandardScaler() \n",
    "# and then fits a model using a random forest regressor. Again, the random_state= parameter can be any number you choose. \n",
    "# It’s simply setting the seed so that you get consistent results each time you run the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Declare hyperparameters to tune.\n",
    "\n",
    "Now it’s time to consider the hyperparameters that we’ll want to tune for our model.\n",
    "\n",
    "### WTF are hyperparameters?\n",
    "\n",
    "There are two types of parameters we need to worry about: model parameters and hyperparameters. \n",
    "\n",
    "Models parameters can be learned directly from the data (i.e. regression coefficients), while hyperparameters cannot.\n",
    "\n",
    "Hyperparameters express “higher-level” structural information about the model, and they are typically set before training the model.\n",
    "\n",
    "**Example: random forest hyperparameters.**\n",
    "\n",
    "As an example, let’s take our random forest for regression:\n",
    "\n",
    "Within each decision tree, the computer can empirically decide where to create branches based on either mean-squared-error (MSE) \n",
    "or mean-absolute-error (MAE). Therefore, the actual branch locations are **model parameters**.\n",
    "\n",
    "However, the algorithm does not know which of the two criteria, MSE or MAE, that it should use. The algorithm also cannot decide\n",
    " how many trees to include in the forest. These are examples of **hyperparameters** that the user must set.\n",
    "\n",
    "We can list the tunable hyperparameters like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('standardscaler', StandardScaler()), ('randomforestregressor', RandomForestRegressor(random_state=123))], 'verbose': False, 'standardscaler': StandardScaler(), 'randomforestregressor': RandomForestRegressor(random_state=123), 'standardscaler__copy': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True, 'randomforestregressor__bootstrap': True, 'randomforestregressor__ccp_alpha': 0.0, 'randomforestregressor__criterion': 'squared_error', 'randomforestregressor__max_depth': None, 'randomforestregressor__max_features': 1.0, 'randomforestregressor__max_leaf_nodes': None, 'randomforestregressor__max_samples': None, 'randomforestregressor__min_impurity_decrease': 0.0, 'randomforestregressor__min_samples_leaf': 1, 'randomforestregressor__min_samples_split': 2, 'randomforestregressor__min_weight_fraction_leaf': 0.0, 'randomforestregressor__n_estimators': 100, 'randomforestregressor__n_jobs': None, 'randomforestregressor__oob_score': False, 'randomforestregressor__random_state': 123, 'randomforestregressor__verbose': 0, 'randomforestregressor__warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print( pipeline.get_params() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also find a list of all the parameters on the RandomForestRegressor documentation page. \n",
    "# Just note that when it’s tuned through a pipeline, you’ll need to prepend  \n",
    "# randomforestregressor__ before the parameter name, like in the code above.\n",
    "\n",
    "# Now, let’s declare the hyperparameters we want to tune through cross-validation.\n",
    "\n",
    "hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                  'randomforestregressor__max_depth': [None, 5, 3, 1]}\n",
    "\n",
    "# As you can see, the format should be a Python dictionary (data structure for key-value pairs) where keys \n",
    "# are the hyperparameter names and values are lists of settings to try. The options for parameter values \n",
    "# can be found on the documentation page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Tune model using a cross-validation pipeline.\n",
    "\n",
    "Now we’re almost ready to dive into fitting our models. But first, we need to spend some time talking about cross-validation.\n",
    "\n",
    "This is one of the most important skills in all of machine learning because it helps you maximize model performance while reducing the chance of overfitting.\n",
    "\n",
    "### What is cross-validation (CV)?\n",
    "\n",
    "Cross-validation is a process for reliably estimating the performance of a method for building a model by training and evaluating your model multiple times using the same method.\n",
    "\n",
    "Practically, that “method” is simply a set of hyperparameters in this context.\n",
    "\n",
    "These are the steps for CV:\n",
    "\n",
    "1. Split your data into k equal parts, or “folds” (typically k=10).\n",
    "2. Train your model on k-1 folds (e.g. the first 9 folds).\n",
    "3. Evaluate it on the remaining “hold-out” fold (e.g. the 10th fold).\n",
    "4. Perform steps (2) and (3) k times, each time holding out a different fold.\n",
    "5. Aggregate the performance across all k folds. This is your performance metric.\n",
    "\n",
    "![K-Fold Cross Validation Diagram](https://elitedatascience.com/wp-content/uploads/2016/12/K-fold_cross_validation_EN.jpg)\n",
    "\n",
    "K-Fold Cross-Validation diagram (Wikipedia)\n",
    "\n",
    "### Why is cross-validation important in machine learning?\n",
    "\n",
    "Let’s say you want to train a random forest regressor. One of the hyperparameters you must tune is the maximum depth allowed for each decision tree in your forest.\n",
    "\n",
    "How can you decide?\n",
    "\n",
    "That’s where cross-validation comes in. Using only your training set, you can use CV to evaluate different hyperparameters and estimate their effectiveness.\n",
    "\n",
    "This allows you to keep your test set “untainted” and save it for a true hold-out evaluation when you’re finally ready to select a model.\n",
    "\n",
    "For example, you can use CV to tune a random forest model, a linear regression model, and a k-nearest neighbors model, using only the training set. \n",
    "Then, you still have the untainted test set to make your final selection between the model families!\n",
    "\n",
    "### So what is a cross-validation “pipeline?”\n",
    "\n",
    "The best practice when performing CV is to include your data preprocessing steps inside the cross-validation loop. This prevents accidentally tainting your training folds with influential data from your test fold.\n",
    "\n",
    "Here’s how the CV pipeline looks after including preprocessing steps:\n",
    "\n",
    "1. Split your data into k equal parts, or “folds” (typically k=10).\n",
    "2. Preprocess k-1 training folds.\n",
    "3. Train your model on the same k-1 folds.\n",
    "4. Preprocess the hold-out fold using the same transformations from step (2).\n",
    "5. Evaluate your model on the same hold-out fold.\n",
    "6. Perform steps (2) – (5) k times, each time holding out a different fold.\n",
    "7. Aggregate the performance across all k folds. This is your performance metric.\n",
    "\n",
    "Fortunately, Scikit-Learn makes it stupidly simple to set this up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edoatley/source/share-predict/.venv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "40 fits failed out of a total of 120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/edoatley/source/share-predict/.venv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/edoatley/source/share-predict/.venv/lib/python3.11/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/edoatley/source/share-predict/.venv/lib/python3.11/site-packages/sklearn/pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/edoatley/source/share-predict/.venv/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/edoatley/source/share-predict/.venv/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/edoatley/source/share-predict/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/edoatley/source/share-predict/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.47699904 0.47699904        nan 0.39015382 0.39015382\n",
      "        nan 0.32935199 0.32935199        nan 0.1729199  0.1729199 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;randomforestregressor&#x27;,\n",
       "                                        RandomForestRegressor(random_state=123))]),\n",
       "             param_grid={&#x27;randomforestregressor__max_depth&#x27;: [None, 5, 3, 1],\n",
       "                         &#x27;randomforestregressor__max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;,\n",
       "                                                                 &#x27;log2&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;randomforestregressor&#x27;,\n",
       "                                        RandomForestRegressor(random_state=123))]),\n",
       "             param_grid={&#x27;randomforestregressor__max_depth&#x27;: [None, 5, 3, 1],\n",
       "                         &#x27;randomforestregressor__max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;,\n",
       "                                                                 &#x27;log2&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;randomforestregressor&#x27;,\n",
       "                 RandomForestRegressor(random_state=123))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=123)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                       ('randomforestregressor',\n",
       "                                        RandomForestRegressor(random_state=123))]),\n",
       "             param_grid={'randomforestregressor__max_depth': [None, 5, 3, 1],\n",
       "                         'randomforestregressor__max_features': ['auto', 'sqrt',\n",
       "                                                                 'log2']})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    " \n",
    "# Fit and tune model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4712595193413647\n",
      "0.34118218749999996\n"
     ]
    }
   ],
   "source": [
    "#@title Step 9: Evaluate model pipeline on test data.\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print( r2_score(y_test, y_pred) )\n",
    "\n",
    "print( mean_squared_error(y_test, y_pred) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so now the question is… is this performance good enough?\n",
    "\n",
    "Well, the rule of thumb is that your very first model probably won’t be the best possible model. However, we recommend a combination of three strategies to decide if you’re satisfied with your model performance.\n",
    "\n",
    "Start with the goal of the model. If the model is tied to a business problem, have you successfully solved the problem?\n",
    "\n",
    "Look in academic literature to get a sense of the current performance benchmarks for specific types of data.\n",
    "\n",
    "Try to find low-hanging fruit in terms of ways to improve your model.\n",
    "\n",
    "There are various ways to improve a model. We’ll have more guides that go into detail about how to improve model performance, but here are a few quick things to try:\n",
    "\n",
    "Try other regression model families (e.g. regularized regression, boosted trees, etc.).\n",
    "\n",
    "Collect more data if it’s cheap to do so.\n",
    "\n",
    "Engineer smarter features after spending more time on exploratory analysis.\n",
    "\n",
    "Speak to a domain expert to get more context (this is a good excuse to go wine tasting!).\n",
    "\n",
    "As a final note, when you try other families of models, we recommend using the same training and test set as you used to fit the random forest model. That’s the best way to get a true apples-to-apples comparison between your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_regressor.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Step 10: Save model for future use.\n",
    "joblib.dump(clf, 'rf_regressor.pkl')\n",
    "\n",
    "# When you want to load the model again, simply use this function:\n",
    "# clf2 = joblib.load('rf_regressor.pkl')\n",
    "# clf2.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
